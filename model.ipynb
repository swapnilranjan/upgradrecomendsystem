{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu2BrePR8LC_"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "class SentimentRecommend:\n",
        "\n",
        "    ROOT_PATH = \"pickle/\"\n",
        "    MODEL_NAME = \"sentiment-classification-xg-boost-model.pkl\"\n",
        "    VECTORIZER = \"tfidf-vectorizer.pkl\"\n",
        "    RECOMMENDER = \"user_final_rating.pkl\"\n",
        "    CLEANED_DATA = \"cleaned-data.pkl\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = pickle.load(open(\n",
        "            SentimentRecommend.ROOT_PATH + SentimentRecommend.MODEL_NAME, 'rb'))\n",
        "        self.vectorizer = pd.read_pickle(\n",
        "            SentimentRecommend.ROOT_PATH + SentimentRecommend.VECTORIZER)\n",
        "        self.user_final_rating = pickle.load(open(\n",
        "            SentimentRecommend.ROOT_PATH + SentimentRecommend.RECOMMENDER, 'rb'))\n",
        "        self.data = pd.read_csv(\"dataset/sample30.csv\")\n",
        "        self.cleaned_data = pickle.load(open(\n",
        "            SentimentRecommend.ROOT_PATH + SentimentRecommend.CLEANED_DATA, 'rb'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    \"\"\"function to get the top product 20 recommendations for the user\"\"\"\n",
        "\n",
        "    def getRecommendationByUser(self, user):\n",
        "        recommedations = []\n",
        "        return list(self.user_final_rating.loc[user].sort_values(ascending=False)[0:20].index)\n",
        "\n",
        "    \"\"\"function to filter the product recommendations using the sentiment model and get the top 5 recommendations\"\"\"\n",
        "\n",
        "    def getSentimentRecommendations(self, user):\n",
        "        if (user in self.user_final_rating.index):\n",
        "            \n",
        "            recommendations = list(\n",
        "                self.user_final_rating.loc[user].sort_values(ascending=False)[0:20].index)\n",
        "            filtered_data = self.cleaned_data[self.cleaned_data.id.isin(\n",
        "                recommendations)]\n",
        "            X = self.vectorizer.transform(\n",
        "                filtered_data[\"reviews_text_cleaned\"].values.astype(str))\n",
        "            filtered_data[\"predicted_sentiment\"] = self.model.predict(X)\n",
        "            temp = filtered_data[['id', 'predicted_sentiment']]\n",
        "            temp_grouped = temp.groupby('id', as_index=False).count()\n",
        "            temp_grouped[\"pos_review_count\"] = temp_grouped.id.apply(lambda x: temp[(\n",
        "                temp.id == x) & (temp.predicted_sentiment == 1)][\"predicted_sentiment\"].count())\n",
        "            temp_grouped[\"total_review_count\"] = temp_grouped['predicted_sentiment']\n",
        "            temp_grouped['pos_sentiment_percent'] = np.round(\n",
        "                temp_grouped[\"pos_review_count\"]/temp_grouped[\"total_review_count\"]*100, 2)\n",
        "            sorted_products = temp_grouped.sort_values(\n",
        "                'pos_sentiment_percent', ascending=False)[0:5]\n",
        "            return pd.merge(self.data, sorted_products, on=\"id\")[[\"name\", \"brand\", \"manufacturer\", \"pos_sentiment_percent\"]].drop_duplicates().sort_values(['pos_sentiment_percent', 'name'], ascending=[False, True])\n",
        "\n",
        "        else:\n",
        "            print(f\"User name {user} doesn't exist\")\n",
        "            return None\n",
        "\n",
        "    \"\"\"function to classify the sentiment to 1/0 - positive or negative - using the trained ML model\"\"\"\n",
        "\n",
        "    def classify_sentiment(self, review_text):\n",
        "        review_text = self.preprocess_text(review_text)\n",
        "        X = self.vectorizer.transform([review_text])\n",
        "        y_pred = self.model.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "    \"\"\"function to preprocess the text before it's sent to ML model\"\"\"\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "\n",
        "        text = text.lower().strip()\n",
        "        text = re.sub(\"\\[\\s*\\w*\\s*\\]\", \"\", text)\n",
        "        dictionary = \"abc\".maketrans('', '', string.punctuation)\n",
        "        text = text.translate(dictionary)\n",
        "        text = re.sub(\"\\S*\\d\\S*\", \"\", text)\n",
        "\n",
        "        \n",
        "        text = self.lemma_text(text)\n",
        "        return text\n",
        "\n",
        "    \"\"\"function to get the pos tag to derive the lemma form\"\"\"\n",
        "\n",
        "    def get_wordnet_pos(self, tag):\n",
        "        if tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return wordnet.NOUN\n",
        "\n",
        "    \"\"\"function to remove the stop words from the text\"\"\"\n",
        "\n",
        "    def remove_stopword(self, text):\n",
        "        words = [word for word in text.split() if word.isalpha()\n",
        "                 and word not in self.stop_words]\n",
        "        return \" \".join(words)\n",
        "\n",
        "    \"\"\"function to derive the base lemma form of the text using the pos tag\"\"\"\n",
        "\n",
        "    def lemma_text(self, text):\n",
        "        word_pos_tags = nltk.pos_tag(word_tokenize(\n",
        "            self.remove_stopword(text)))  \n",
        "        \n",
        "        words = [self.lemmatizer.lemmatize(tag[0], self.get_wordnet_pos(\n",
        "            tag[1])) for idx, tag in enumerate(word_pos_tags)]\n",
        "        return \" \".join(words)"
      ]
    }
  ]
}