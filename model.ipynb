{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRdMDgi_P0YV"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus.reader import reviews\n",
        "import pandas as pd\n",
        "import re, nltk, spacy, string\n",
        "import en_core_web_sm\n",
        "import pickle as pk\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# load the pickle files \n",
        "count_vector = pk.load(open('pickle_file/count_vector.pkl','rb'))            # Count Vectorizer\n",
        "tfidf_transformer = pk.load(open('pickle_file/tfidf_transformer.pkl','rb')) # TFIDF Transformer\n",
        "model = pk.load(open('pickle_file/model.pkl','rb'))                          # Classification Model\n",
        "recommend_matrix = pk.load(open('pickle_file/user_final_rating.pkl','rb'))   # User-User Recommendation System \n",
        "\n",
        "nlp = spacy.load('en_core_web_sm',disable=['ner','parser'])\n",
        "\n",
        "product_df = pd.read_csv('sample30.csv',sep=\",\")\n",
        "\n",
        "\n",
        "# special_characters removal\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    \"\"\"Remove the special Characters\"\"\"\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation_and_splchars(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_word = remove_special_characters(new_word, True)\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "stopword_list= stopwords.words('english')\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopword_list:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def stem_words(words):\n",
        "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "def normalize(words):\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation_and_splchars(words)\n",
        "    words = remove_stopwords(words)\n",
        "    return words\n",
        "\n",
        "def lemmatize(words):\n",
        "    lemmas = lemmatize_verbs(words)\n",
        "    return lemmas\n",
        "\n",
        "#predicting the sentiment of the product review comments\n",
        "def model_predict(text):\n",
        "    word_vector = count_vector.transform(text)\n",
        "    tfidf_vector = tfidf_transformer.transform(word_vector)\n",
        "    output = model.predict(tfidf_vector)\n",
        "    return output\n",
        "\n",
        "def normalize_and_lemmaize(input_text):\n",
        "    input_text = remove_special_characters(input_text)\n",
        "    words = nltk.word_tokenize(input_text)\n",
        "    words = normalize(words)\n",
        "    lemmas = lemmatize(words)\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "#Recommend the products based on the sentiment from model\n",
        "def recommend_products(user_name):\n",
        "    recommend_matrix = pk.load(open('pickle_file/user_final_rating.pkl','rb'))\n",
        "    product_list = pd.DataFrame(recommend_matrix.loc[user_name].sort_values(ascending=False)[0:20])\n",
        "    product_frame = product_df[product_df.name.isin(product_list.index.tolist())]\n",
        "    output_df = product_frame[['name','reviews_text']]\n",
        "    output_df['lemmatized_text'] = output_df['reviews_text'].map(lambda text: normalize_and_lemmaize(text))\n",
        "    output_df['predicted_sentiment'] = model_predict(output_df['lemmatized_text'])\n",
        "    return output_df\n",
        "    \n",
        "def top5_products(df):\n",
        "    total_product=df.groupby(['name']).agg('count')\n",
        "    rec_df = df.groupby(['name','predicted_sentiment']).agg('count')\n",
        "    rec_df=rec_df.reset_index()\n",
        "    merge_df = pd.merge(rec_df,total_product['reviews_text'],on='name')\n",
        "    merge_df['%percentage'] = (merge_df['reviews_text_x']/merge_df['reviews_text_y'])*100\n",
        "    merge_df=merge_df.sort_values(ascending=False,by='%percentage')\n",
        "    output_products = pd.DataFrame(merge_df['name'][merge_df['predicted_sentiment'] ==  1][:5])\n",
        "    return output_products"
      ]
    }
  ]
}